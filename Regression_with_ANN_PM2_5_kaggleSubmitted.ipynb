{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajgit-123/MyProject/blob/master/Regression_with_ANN_PM2_5_kaggleSubmitted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clrePHUhNqBA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Preprocessing\n",
        "# Load data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "abT-Yk5iN7b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.rename(columns={'Unnamed: 0':'ID'}, inplace=True)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "TD5bA636SMHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing or anomalous values\n",
        "train_data = train_data.dropna()"
      ],
      "metadata": {
        "id": "yTyjfYSVN8p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = ['SO2', 'NO2', 'CO', 'O3', 'Temp', 'Press', 'DewP', 'Rain', 'WinSpeed']  # Assuming these are numerical\n",
        "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
        "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])  # Use the same scaler for test data\n"
      ],
      "metadata": {
        "id": "d-EqcGMtO9ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features if present (e.g., 'Station' and 'WinDir')\n",
        "train_data = pd.get_dummies(train_data, columns=['Station', 'WinDir'])\n",
        "test_data = pd.get_dummies(test_data, columns=['Station', 'WinDir'])\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data.drop(columns=['PM2.5']), train_data['PM2.5'], test_size=0.2, random_state=42)\n",
        "X_test = test_data  # No labels for test data\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "AewXDH1JRw-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Model Architecture\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MT8FHIS9R0fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Training the Model\n",
        "def train(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs=100, batch_size=64):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_train[i:i+batch_size])\n",
        "            loss = criterion(outputs.squeeze(), y_train[i:i+batch_size])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs.squeeze(), y_val)\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')"
      ],
      "metadata": {
        "id": "StHN2XBAR4Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "model = ANN(input_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train(model, criterion, optimizer, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QLosLPXR7Nw",
        "outputId": "f26f1b0f-358c-4eea-f2be-c1f2f80df27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 9894.9287109375, Val Loss: 7791.39501953125\n",
            "Epoch 2/100, Loss: 8451.46484375, Val Loss: 6926.37646484375\n",
            "Epoch 3/100, Loss: 6125.14013671875, Val Loss: 5736.28662109375\n",
            "Epoch 4/100, Loss: 8476.4033203125, Val Loss: 6108.65966796875\n",
            "Epoch 5/100, Loss: 3937.06103515625, Val Loss: 4807.16845703125\n",
            "Epoch 6/100, Loss: 4819.40673828125, Val Loss: 3287.059326171875\n",
            "Epoch 7/100, Loss: 2141.216796875, Val Loss: 3918.288818359375\n",
            "Epoch 8/100, Loss: 2085.848876953125, Val Loss: 2494.140380859375\n",
            "Epoch 9/100, Loss: 2214.25390625, Val Loss: 2509.012451171875\n",
            "Epoch 10/100, Loss: 2103.810302734375, Val Loss: 2521.751708984375\n",
            "Epoch 11/100, Loss: 1861.76904296875, Val Loss: 2607.4833984375\n",
            "Epoch 12/100, Loss: 1664.718017578125, Val Loss: 2891.29052734375\n",
            "Epoch 13/100, Loss: 1804.4295654296875, Val Loss: 2733.58740234375\n",
            "Epoch 14/100, Loss: 1742.0772705078125, Val Loss: 2272.525634765625\n",
            "Epoch 15/100, Loss: 1483.044677734375, Val Loss: 2312.58203125\n",
            "Epoch 16/100, Loss: 1457.208740234375, Val Loss: 2375.572021484375\n",
            "Epoch 17/100, Loss: 1411.5062255859375, Val Loss: 2200.7978515625\n",
            "Epoch 18/100, Loss: 1438.04833984375, Val Loss: 2168.624755859375\n",
            "Epoch 19/100, Loss: 1384.0478515625, Val Loss: 2109.4111328125\n",
            "Epoch 20/100, Loss: 1546.6923828125, Val Loss: 2200.0546875\n",
            "Epoch 21/100, Loss: 1363.505859375, Val Loss: 2195.0673828125\n",
            "Epoch 22/100, Loss: 1454.2159423828125, Val Loss: 2286.19482421875\n",
            "Epoch 23/100, Loss: 1402.010009765625, Val Loss: 2137.8603515625\n",
            "Epoch 24/100, Loss: 1507.9666748046875, Val Loss: 2168.735595703125\n",
            "Epoch 25/100, Loss: 2003.697265625, Val Loss: 2001.4571533203125\n",
            "Epoch 26/100, Loss: 8606.3232421875, Val Loss: 5953.62841796875\n",
            "Epoch 27/100, Loss: 1413.286865234375, Val Loss: 2184.35009765625\n",
            "Epoch 28/100, Loss: 1334.8336181640625, Val Loss: 2140.33447265625\n",
            "Epoch 29/100, Loss: 1375.5999755859375, Val Loss: 2247.92431640625\n",
            "Epoch 30/100, Loss: 1352.67236328125, Val Loss: 2234.637451171875\n",
            "Epoch 31/100, Loss: 1396.621337890625, Val Loss: 2131.796630859375\n",
            "Epoch 32/100, Loss: 1588.145263671875, Val Loss: 2030.5667724609375\n",
            "Epoch 33/100, Loss: 1548.798095703125, Val Loss: 1984.6097412109375\n",
            "Epoch 34/100, Loss: 1639.7384033203125, Val Loss: 1940.5560302734375\n",
            "Epoch 35/100, Loss: 1597.6407470703125, Val Loss: 1953.3905029296875\n",
            "Epoch 36/100, Loss: 1882.003173828125, Val Loss: 1993.9129638671875\n",
            "Epoch 37/100, Loss: 1597.0543212890625, Val Loss: 1939.24072265625\n",
            "Epoch 38/100, Loss: 1596.9151611328125, Val Loss: 1958.4217529296875\n",
            "Epoch 39/100, Loss: 1752.025634765625, Val Loss: 1945.4798583984375\n",
            "Epoch 40/100, Loss: 1788.78564453125, Val Loss: 1935.2696533203125\n",
            "Epoch 41/100, Loss: 1812.513916015625, Val Loss: 1947.733154296875\n",
            "Epoch 42/100, Loss: 1764.54736328125, Val Loss: 1921.6365966796875\n",
            "Epoch 43/100, Loss: 1436.0035400390625, Val Loss: 2098.83935546875\n",
            "Epoch 44/100, Loss: 1527.289794921875, Val Loss: 1943.2835693359375\n",
            "Epoch 45/100, Loss: 1644.9893798828125, Val Loss: 1928.135009765625\n",
            "Epoch 46/100, Loss: 1690.27880859375, Val Loss: 1918.014892578125\n",
            "Epoch 47/100, Loss: 1522.097900390625, Val Loss: 1925.01611328125\n",
            "Epoch 48/100, Loss: 1431.8651123046875, Val Loss: 1924.9609375\n",
            "Epoch 49/100, Loss: 1671.0067138671875, Val Loss: 1916.93701171875\n",
            "Epoch 50/100, Loss: 1589.298828125, Val Loss: 1930.821533203125\n",
            "Epoch 51/100, Loss: 1488.2803955078125, Val Loss: 1918.5718994140625\n",
            "Epoch 52/100, Loss: 1609.986083984375, Val Loss: 1935.555908203125\n",
            "Epoch 53/100, Loss: 1542.4305419921875, Val Loss: 1945.3409423828125\n",
            "Epoch 54/100, Loss: 1522.98193359375, Val Loss: 1936.947265625\n",
            "Epoch 55/100, Loss: 1577.602783203125, Val Loss: 1948.6339111328125\n",
            "Epoch 56/100, Loss: 1619.83837890625, Val Loss: 1949.2041015625\n",
            "Epoch 57/100, Loss: 1560.6328125, Val Loss: 1931.2591552734375\n",
            "Epoch 58/100, Loss: 1457.048828125, Val Loss: 1932.580078125\n",
            "Epoch 59/100, Loss: 1528.14892578125, Val Loss: 1950.2303466796875\n",
            "Epoch 60/100, Loss: 1429.612060546875, Val Loss: 1923.88623046875\n",
            "Epoch 61/100, Loss: 1467.5008544921875, Val Loss: 1927.043212890625\n",
            "Epoch 62/100, Loss: 1411.454345703125, Val Loss: 1920.409912109375\n",
            "Epoch 63/100, Loss: 1412.3175048828125, Val Loss: 1936.1820068359375\n",
            "Epoch 64/100, Loss: 1954.35595703125, Val Loss: 1970.19140625\n",
            "Epoch 65/100, Loss: 1718.0654296875, Val Loss: 1988.4114990234375\n",
            "Epoch 66/100, Loss: 1730.581787109375, Val Loss: 2065.74755859375\n",
            "Epoch 67/100, Loss: 1586.7984619140625, Val Loss: 1943.578857421875\n",
            "Epoch 68/100, Loss: 1510.578857421875, Val Loss: 2001.353759765625\n",
            "Epoch 69/100, Loss: 1565.4481201171875, Val Loss: 2011.7164306640625\n",
            "Epoch 70/100, Loss: 1558.621337890625, Val Loss: 1911.408447265625\n",
            "Epoch 71/100, Loss: 1349.98974609375, Val Loss: 1938.452880859375\n",
            "Epoch 72/100, Loss: 1519.3502197265625, Val Loss: 1908.0513916015625\n",
            "Epoch 73/100, Loss: 1758.3079833984375, Val Loss: 1931.1392822265625\n",
            "Epoch 74/100, Loss: 1582.0477294921875, Val Loss: 1938.988037109375\n",
            "Epoch 75/100, Loss: 1518.6842041015625, Val Loss: 1939.5706787109375\n",
            "Epoch 76/100, Loss: 1530.682861328125, Val Loss: 2058.98388671875\n",
            "Epoch 77/100, Loss: 1575.8922119140625, Val Loss: 1923.2503662109375\n",
            "Epoch 78/100, Loss: 1466.75732421875, Val Loss: 1944.99462890625\n",
            "Epoch 79/100, Loss: 1311.858642578125, Val Loss: 1949.40283203125\n",
            "Epoch 80/100, Loss: 1431.490966796875, Val Loss: 1934.75439453125\n",
            "Epoch 81/100, Loss: 1662.6767578125, Val Loss: 1962.658935546875\n",
            "Epoch 82/100, Loss: 1660.3984375, Val Loss: 1956.9473876953125\n",
            "Epoch 83/100, Loss: 1505.222412109375, Val Loss: 1972.126220703125\n",
            "Epoch 84/100, Loss: 1420.9866943359375, Val Loss: 1911.642822265625\n",
            "Epoch 85/100, Loss: 1289.7275390625, Val Loss: 1932.0369873046875\n",
            "Epoch 86/100, Loss: 1632.6114501953125, Val Loss: 2132.179443359375\n",
            "Epoch 87/100, Loss: 1281.201904296875, Val Loss: 2060.0166015625\n",
            "Epoch 88/100, Loss: 1490.673583984375, Val Loss: 1991.056396484375\n",
            "Epoch 89/100, Loss: 1426.9256591796875, Val Loss: 1924.2069091796875\n",
            "Epoch 90/100, Loss: 1295.0794677734375, Val Loss: 1928.04052734375\n",
            "Epoch 91/100, Loss: 1424.30126953125, Val Loss: 1912.9755859375\n",
            "Epoch 92/100, Loss: 3364.2763671875, Val Loss: 4247.794921875\n",
            "Epoch 93/100, Loss: 1505.8135986328125, Val Loss: 1958.782470703125\n",
            "Epoch 94/100, Loss: 1522.5001220703125, Val Loss: 2006.82958984375\n",
            "Epoch 95/100, Loss: 1519.0167236328125, Val Loss: 1948.13330078125\n",
            "Epoch 96/100, Loss: 1478.6251220703125, Val Loss: 1915.610107421875\n",
            "Epoch 97/100, Loss: 1482.9781494140625, Val Loss: 1961.9066162109375\n",
            "Epoch 98/100, Loss: 1386.7969970703125, Val Loss: 1919.177001953125\n",
            "Epoch 99/100, Loss: 1356.7979736328125, Val Loss: 1911.6365966796875\n",
            "Epoch 100/100, Loss: 8661.8291015625, Val Loss: 6468.89599609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    # Since we don't have ground truth for the test set, we cannot calculate RMSE or other metrics"
      ],
      "metadata": {
        "id": "Su11vc69SAt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate predictions for submission\n",
        "test_ids = range(len(X_test))\n",
        "\n",
        "submission_df = pd.DataFrame({'ID': test_ids,'PM2.5': test_outputs.squeeze().numpy()})\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "EO6sbaI_PHpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}